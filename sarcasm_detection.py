# -*- coding: utf-8 -*-
"""Sarcasm Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jmTwyXSXcpyM2p7vRv4JepXmmS3C8CpN
"""

import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import json, os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Data Preprocessing
import re, nltk, string
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')


punc = list(string.punctuation)
stop_words = stopwords.words("english")
lemma = WordNetLemmatizer()
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.utils import to_categorical
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences




# Building Model
from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, GlobalAveragePooling1D
import tensorflow.keras as k

data = pd.read_json(r"/Sarcasm_Headlines_Dataset.json", lines=True)
data.head()

data.drop(columns="article_link", axis=1, inplace=True)
data.head(5)

data["headline"][0]

from google.colab import drive
drive.mount('/content/drive')

data.info()

punc = list(string.punctuation)
stop_words = stopwords.words("english")
lemma = WordNetLemmatizer()

def Process(data):
    data.lower()

    data = " ".join([lemma.lemmatize(word) for word in word_tokenize(data) if ((word not in punc) and (word not in stop_words))])

    data = re.sub("[^a-z]", " ", data)

    return data

data = pd.DataFrame({"headline": ["Your", "Headline", "Data"]})

def Process(input_data):
    processed_data = input_data.upper()
    return processed_data

data["headline"] = data["headline"].apply(Process)
data.head(5)

label_column_name = "is_sarcastic"

if label_column_name in data.columns:
    labels = to_categorical(data[label_column_name], num_classes=2)
    print(labels[:5])
else:
    print(f"Column '{label_column_name}' not found in the DataFrame.")

# Now, you can define 'X' and use 'label'
X = data["headline"]

# Check if 'label' is defined before assigning it to 'Y'
if 'label' in locals() or 'label' in globals():
    Y = label
    # Print the first 2 elements of 'Y'
    print(Y[:2])
else:
    print("Variable 'label' is not defined.")

tokenize = Tokenizer(oov_token="<oov>")
tokenize.fit_on_texts(X)
word_idx = tokenize.word_index

data_seqence = tokenize.texts_to_sequences(X)
pad_seq = pad_sequences(data_seqence, padding="pre", truncating="pre")

print("The Padding Sequance Shape is  --> ", pad_seq.shape)

input_length = max(len(seq) for seq in data_seqence)

vocabulary_size = len(word_idx) + 1

input_length, vocabulary_size

data = pd.DataFrame({
    "headline": ["This is a positive headline", "Not a sarcastic one", "Really funny title"],
    "is_sarcastic": [0, 0, 1]  # 0 for non-sarcastic, 1 for sarcastic
})# Preprocess headlines
max_words = 1000  # Example: consider the top 1000 words
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(data["headline"])
sequences = tokenizer.texts_to_sequences(data["headline"])
pad_seq = pad_sequences(sequences, maxlen=20, padding="post", truncating="post")

# One-hot encode labels
label = to_categorical(data["is_sarcastic"], num_classes=2)

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(pad_seq, label, train_size=0.7)

model = k.models.Sequential([
    Embedding(vocabulary_size, 50, input_length=input_length),
    GlobalAveragePooling1D(),
    Dense(48, activation="relu"),
    Dense(2, activation="softmax")
])

model.compile(optimizer="adam", loss=k.losses.BinaryCrossentropy(), metrics=["accuracy"])

# Find the maximum length of sequences in your data
maxlen = max(len(seq) for seq in sequences)

# Tokenize and pad sequences
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(data["headline"])
sequences = tokenizer.texts_to_sequences(data["headline"])
pad_seq = pad_sequences(sequences, maxlen=maxlen, padding="post", truncating="post")

# Assuming you have a label column 'is_sarcastic' in your 'data' DataFrame
label = to_categorical(data["is_sarcastic"], 2)

# Split your data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(pad_seq, label, train_size=0.7)

# Define your model
embedding_dim = 50  # Adjust this value based on your preference or task requirements
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))
model.add(Flatten())  # Flatten layer for simplicity, adjust as needed
model.add(Dense(2, activation='softmax'))  # Output layer, assuming 2 classes

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test), verbose=2)

# ... (previous code to define 'label' and split the data)

# Define your model
embedding_dim = 50
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model and capture the training history
history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test), verbose=2)

# Plot the loss values
plt.plot(history.history["loss"], label="Loss")
plt.plot(history.history["val_loss"], label="Val_Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Vs Epochs")
plt.legend()
plt.grid()
plt.show()

plt.plot(history.history["accuracy"], label="accuracy")
plt.plot(history.history["val_accuracy"], label="val_accuracy")

plt.xlabel("Epochs")
plt.ylabel("Accuracy")

plt.title("Accuracy Vs Epochs")

plt.legend()
plt.grid()

# Example input text
input_text = "This is a test sentence. It's not meant to be taken seriously."

# Tokenize and preprocess the input text
text = word_tokenize(input_text)
new_text = ""
for word in text:
    if (word not in stop_words) and (word not in punc):
        new_text += lemma.lemmatize(word)
        new_text += " "

# Convert the preprocessed text to sequence and pad it
test_sequence = tokenizer.texts_to_sequences([new_text])
# Assuming your model was trained with maxlen=5
maxlen = 5
test_padding = pad_sequences(test_sequence, maxlen=maxlen, padding="pre", truncating="pre")

# Make the prediction
prediction = model.predict(test_padding)

# Print the prediction
print(prediction[0])
if np.argmax(prediction) == 0:
    print("This Message is --> is_sarcastic")
else:
    print("This Message is --> not is_sarcastic")

